{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.layers import Dot, Embedding,Activation, Input, Reshape, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Dense, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input(lab):\n",
    "    sents = []\n",
    "    with open(\"trail0/\"+str(lab)+\".csv\", 'r') as sent_file:\n",
    "        for line in sent_file:\n",
    "            sents.append(re.sub(\"\\n\",\"\",line))\n",
    "    labels = [lab]*len(sents)\n",
    "    return sents, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, labels = [], []\n",
    "\n",
    "for k in range(19):\n",
    "    s,l = data_input(k)\n",
    "    sents += s\n",
    "    labels+= l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576 words in vocab\n",
      "(1048576, 100) embedding matrix\n"
     ]
    }
   ],
   "source": [
    "vocab_fn = \"word_list.txt\"\n",
    "with open(vocab_fn, 'r') as vfn:\n",
    "    index2word = vfn.read().split('\\n')\n",
    "index2word = index2word[:-1]\n",
    "print(len(index2word),\"words in vocab\")\n",
    "\n",
    "mat_fn = \"weight.npy\"\n",
    "embedding_mat = np.load(mat_fn)\n",
    "print(embedding_mat.shape,\"embedding matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add NULL (0) and UNK to our vocab\n",
    "lookup_with_unk = {word:i+2 for i,word in enumerate(index2word)}\n",
    "UNK_IND = 1\n",
    "\n",
    "#add null and UNK vectors to our embedding matrix so it still lines up\n",
    "embeddings_with_unk = np.zeros((embedding_mat.shape[0]+2, embedding_mat.shape[1]))\n",
    "embeddings_with_unk[2:] = embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_len = 20\n",
    "X_matrix = np.zeros((len(sents), sent_len), dtype=np.int32)\n",
    "for i,sent in enumerate(sents):\n",
    "    sent_tokens = sent.strip().lower().split() #lazy tokenization\n",
    "    sent_inds = [lookup_with_unk[s] if s in lookup_with_unk else UNK_IND for s in sent_tokens]\n",
    "    sent_inds = sent_inds[:sent_len] #truncate if necessary\n",
    "    X_matrix[i, :len(sent_inds)] = sent_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y, 19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 0\n",
    "hidden_size = 16\n",
    "vocab_size,embed_size = embeddings_with_unk.shape\n",
    "\n",
    "#simplest possible model\n",
    "\n",
    "sent_in = Input((None,), dtype=\"int32\", name=\"sent_in\")\n",
    "#load the weights into the model\n",
    "embed_layer = Embedding(vocab_size, embed_size, name=\"word_vec\", weights=[embeddings_with_unk,])\n",
    "sent_embeddings = embed_layer(sent_in)\n",
    "\n",
    "sent_embeddings = Dropout(0.25)(sent_embeddings)\n",
    "\n",
    "#compose the words by averaging their vectors\n",
    "#a recurrent layer would be much more common here, but we're keeping this extremely simple\n",
    "sent_avg = GlobalAveragePooling1D()(sent_embeddings)\n",
    "\n",
    "#add a fully-connected layer - in practice, we would want to see whether this actually helps or not\n",
    "hidden_repr = Dense(hidden_size, activation=\"tanh\", name=\"tanh\")(sent_avg)\n",
    "\n",
    "pred = Dense(19, activation=\"softmax\", name=\"softmax\")(hidden_repr)\n",
    "sentiment_model = Model(inputs=[sent_in], outputs=[pred,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiahao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:92: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 104857800 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91200 samples, validate on 22800 samples\n",
      "Epoch 1/2\n",
      "91200/91200 [==============================] - 2857s 31ms/step - loss: 0.1823 - acc: 0.9474 - val_loss: 0.4568 - val_acc: 0.9471\n",
      "Epoch 2/2\n",
      "91200/91200 [==============================] - 2855s 31ms/step - loss: 0.1715 - acc: 0.9477 - val_loss: 0.5305 - val_acc: 0.9461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08df58efd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",])\n",
    "sentiment_model.fit(X_matrix,y, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "from keras.models import model_from_json\n",
    "model_json = sentiment_model.to_json()\n",
    "with open(\"new_model_0.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "sentiment_model.save_weights(\"model_0.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Bidirectional, Embedding, Dense, Dropout, SpatialDropout1D, LSTM, Activation\n",
    "from keras.layers.merge import concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "hidden_size = 16\n",
    "vocab_size,embed_size = embeddings_with_unk.shape\n",
    "\n",
    "#simplest possible model\n",
    "\n",
    "sent_in = Input((20,), dtype=\"int32\", name=\"sent_in\")\n",
    "#load the weights into the model\n",
    "embed_layer = Embedding(vocab_size, embed_size, name=\"word_vec\", weights=[embeddings_with_unk,])\n",
    "x = embed_layer(sent_in)\n",
    "x = Activation('tanh')(x)\n",
    "embed_drop = SpatialDropout1D(0.25, name='embed_drop')\n",
    "x = embed_drop(x)\n",
    "lstm_0_output = Bidirectional(LSTM(512, return_sequences=True), name=\"bi_lstm_0\")(x)\n",
    "lstm_1_output = Bidirectional(LSTM(512, return_sequences=True), name=\"bi_lstm_1\")(lstm_0_output)\n",
    "x = concatenate([lstm_1_output, lstm_0_output, x])\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "pred = Dense(19, activation=\"softmax\", name=\"softmax\")(x)\n",
    "sentiment_model = Model(inputs=[sent_in], outputs=[pred,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiahao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:92: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 104857800 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91200 samples, validate on 22800 samples\n",
      "Epoch 1/2\n",
      "91200/91200 [==============================] - 4900s 54ms/step - loss: 0.1749 - acc: 0.9476 - val_loss: 0.5637 - val_acc: 0.9451\n",
      "Epoch 2/2\n",
      "91200/91200 [==============================] - 4895s 54ms/step - loss: 0.1657 - acc: 0.9481 - val_loss: 0.6414 - val_acc: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08cdd2ccf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",])\n",
    "sentiment_model.fit(X_matrix,y, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "from keras.models import model_from_json\n",
    "model_json = sentiment_model.to_json()\n",
    "with open(\"new_model_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "sentiment_model.save_weights(\"new_model_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input(lab):\n",
    "    sents = []\n",
    "    with open(str(lab)+\".csv\", 'r') as sent_file:\n",
    "        for line in sent_file:\n",
    "            sents.append(re.sub(\"\\n\",\"\",line))\n",
    "    labels = [lab]*len(sents)\n",
    "    return sents, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents, labels = [], []\n",
    "\n",
    "for k in range(19):\n",
    "    s,l = data_input(k)\n",
    "    sents += s\n",
    "    labels+= l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576 words in vocab\n",
      "(1048576, 100) embedding matrix\n"
     ]
    }
   ],
   "source": [
    "vocab_fn = \"word_list.txt\"\n",
    "with open(vocab_fn, 'r') as vfn:\n",
    "    index2word = vfn.read().split('\\n')\n",
    "index2word = index2word[:-1]\n",
    "print(len(index2word),\"words in vocab\")\n",
    "\n",
    "mat_fn = \"weight.npy\"\n",
    "embedding_mat = np.load(mat_fn)\n",
    "print(embedding_mat.shape,\"embedding matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add NULL (0) and UNK to our vocab\n",
    "lookup_with_unk = {word:i+2 for i,word in enumerate(index2word)}\n",
    "UNK_IND = 1\n",
    "\n",
    "#add null and UNK vectors to our embedding matrix so it still lines up\n",
    "embeddings_with_unk = np.zeros((embedding_mat.shape[0]+2, embedding_mat.shape[1]))\n",
    "embeddings_with_unk[2:] = embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_len = 20\n",
    "X_matrix = np.zeros((len(sents), sent_len), dtype=np.int32)\n",
    "for i,sent in enumerate(sents):\n",
    "    sent_tokens = sent.strip().lower().split() #lazy tokenization\n",
    "    sent_inds = [lookup_with_unk[s] if s in lookup_with_unk else UNK_IND for s in sent_tokens]\n",
    "    sent_inds = sent_inds[:sent_len] #truncate if necessary\n",
    "    X_matrix[i, :len(sent_inds)] = sent_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.asarray(labels)\n",
    "y = np_utils.to_categorical(y, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 0\n",
    "hidden_size = 16\n",
    "vocab_size,embed_size = embeddings_with_unk.shape\n",
    "\n",
    "#simplest possible model\n",
    "\n",
    "sent_in = Input((None,), dtype=\"int32\", name=\"sent_in\")\n",
    "#load the weights into the model\n",
    "embed_layer = Embedding(vocab_size, embed_size, name=\"word_vec\", weights=[embeddings_with_unk,])\n",
    "sent_embeddings = embed_layer(sent_in)\n",
    "\n",
    "sent_embeddings = Dropout(0.25)(sent_embeddings)\n",
    "\n",
    "#compose the words by averaging their vectors\n",
    "#a recurrent layer would be much more common here, but we're keeping this extremely simple\n",
    "sent_avg = GlobalAveragePooling1D()(sent_embeddings)\n",
    "\n",
    "#add a fully-connected layer - in practice, we would want to see whether this actually helps or not\n",
    "hidden_repr = Dense(hidden_size, activation=\"tanh\", name=\"tanh\")(sent_avg)\n",
    "\n",
    "pred = Dense(19, activation=\"softmax\", name=\"softmax\")(hidden_repr)\n",
    "sentiment_model = Model(inputs=[sent_in], outputs=[pred,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiahao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:92: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 104857800 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 182400 samples, validate on 45600 samples\n",
      "Epoch 1/2\n",
      "182400/182400 [==============================] - 5336s 29ms/step - loss: 0.1779 - acc: 0.9475 - val_loss: 0.5082 - val_acc: 0.9466\n",
      "Epoch 2/2\n",
      "182400/182400 [==============================] - 5311s 29ms/step - loss: 0.1681 - acc: 0.9479 - val_loss: 0.6757 - val_acc: 0.9457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08cd7c4940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",])\n",
    "sentiment_model.fit(X_matrix,y, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "from keras.models import model_from_json\n",
    "model_json = sentiment_model.to_json()\n",
    "with open(\"new_model_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "sentiment_model.save_weights(\"model_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "hidden_size = 16\n",
    "vocab_size,embed_size = embeddings_with_unk.shape\n",
    "\n",
    "#simplest possible model\n",
    "\n",
    "sent_in = Input((20,), dtype=\"int32\", name=\"sent_in\")\n",
    "#load the weights into the model\n",
    "embed_layer = Embedding(vocab_size, embed_size, name=\"word_vec\", weights=[embeddings_with_unk,])\n",
    "x = embed_layer(sent_in)\n",
    "x = Activation('tanh')(x)\n",
    "embed_drop = SpatialDropout1D(0.25, name='embed_drop')\n",
    "x = embed_drop(x)\n",
    "lstm_0_output = Bidirectional(LSTM(512, return_sequences=True), name=\"bi_lstm_0\")(x)\n",
    "lstm_1_output = Bidirectional(LSTM(512, return_sequences=True), name=\"bi_lstm_1\")(lstm_0_output)\n",
    "x = concatenate([lstm_1_output, lstm_0_output, x])\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "pred = Dense(19, activation=\"softmax\", name=\"softmax\")(x)\n",
    "sentiment_model = Model(inputs=[sent_in], outputs=[pred,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiahao/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:92: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 104857800 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 182400 samples, validate on 45600 samples\n",
      "Epoch 1/2\n",
      "182400/182400 [==============================] - 9385s 51ms/step - loss: 0.1718 - acc: 0.9478 - val_loss: 0.6409 - val_acc: 0.9446\n",
      "Epoch 2/2\n",
      "182400/182400 [==============================] - 9373s 51ms/step - loss: 0.1629 - acc: 0.9484 - val_loss: 0.7285 - val_acc: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f094ff21518>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",])\n",
    "sentiment_model.fit(X_matrix,y, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "from keras.models import model_from_json\n",
    "model_json = sentiment_model.to_json()\n",
    "with open(\"new_model_3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "sentiment_model.save_weights(\"new_model_3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
